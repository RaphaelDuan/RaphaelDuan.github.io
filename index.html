<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Di Duan</title>

    <meta name="author" content="Di Duan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Di Duan
                </p>
                <p>I‚Äôm a first second third fourth year Ph.D. candidate at the <a href="https://www.cs.cityu.edu.hk/">Department of Computer Science</a> at <a href="https://www.cityu.edu.hk/">City University of Hong Kong</a> (supervised by Dr. <a href="https://www.weitaoxu.com/">Weitao Xu</a>, co-supervised by Prof. <a href="https://www.cs.cityu.edu.hk/~jia/">Xiaohua Jia</a>), I also work closely with Dr. <a href="https://www.cse.msu.edu/~litianx2/">Tianxing Li</a>. Before that, I obtained my M.Sc. degree from the Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk/">HKUST</a>) in 2019, and my B.E. degree from the Harbin Engineering University (<a href="http://www.hrbeu.edu.cn/">HEU</a>) in 2018, respectively.
                </p>
                <p>My research lies at the intersection of Mobile Computing and Human-Computer Interaction ‚Äî with a special focus on system-enabled novel applications, wearable sensing, and deep learning. I received the <a href="https://www.percom.org/">IEEE PerCom 2023</a> Mark Weiser Best Paper Award.
                </p>
                <p>
                  <strong style="color: red;">I'm actively seeking postdoctoral positions</strong> with a focus on <strong>system-enabled novel applications</strong> and <strong>human-computer interaction</strong>.
                </p>
                <p>
                  <strong>Email:</strong> duandiacademic [AT] gmail [DOT] com
                </p>
                <p style="text-align:center">
                  <a href="mailto:duandiacademic@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/DiDuan-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BRFlCnUAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/di-duan-543065170/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/pictures/avatar.jpg"><img style="width:80%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/pictures/avatar.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li>[2024/03] Our paper F2Key is conditionally accepted by MobiSys'24. I will present this paper in Tokyo in Jun. 2024.</li>
                  <li>[2024/02] One co-authored paper RF-Egg is accepted by MobiCom'24.</li>
                  <li>[2023/10] Our paper EarSE is accepted by IMWUT/UbiComp'24. I will present this paper in Melbourne in Oct. 2024.</li>
                  <li>[2023/03] Receive Monetary Award of 300 USD from Elsevier.</li>
                  <li>[2023/03] Our paper EMGSense receives the <span class="highlight">Mark Weiser Best Paper Award</span> at PerCom 2023.</li>
                </ul>

                <h2>Publications</h2>
                <ul class="b">
                  <li style="margin-bottom: 8px;">EarSE: Bringing Robust Speech Enhancement to COTS Headphones<br>
                    <u><b>Di Duan</b></u>, Yongliang Chen, Weitao Xu, Tianxing Li<br>
                      <i>ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (<b>IMWUT/UbiComp 2024</b>)</i>
                  </li>

                  <li style="margin-bottom: 8px;">EMGSense: A Low-Effort Self-Supervised Domain Adaptation Framework for EMG Sensing<br>
                    <u><b>Di Duan</b></u>, Huanqi Yang, Guohao Lan, Tianxing Li, Xiaohua Jia, Weitao Xu<br>
                      <i>The 21st International Conference on Pervasive Computing and Communications (<b>PerCom 2023</b>)</i><br>
                      <strong>accpetance rate: 16.9%</strong> <strong style="color: red;">üèÜ Mark Weiser Best Paper Award (1/157)</strong> 
                  </li>

                  <li style="margin-bottom: 8px;">RF-Egg: An RF Solution for Fine-Grained Multi-Target and Multi-Task Egg Incubation Sensing<br>
                    Zehua Sun, Tao Ni, Yongliang Chen, <u><b>Di Duan</b></u>, Kai Liu, Weitao Xu<br>
                      <i>The 30th Annual International Conference On Mobile Computing And Networking (<b>MobiCom 2024</b>)</i>
                  </li>

                  <li style="margin-bottom: 8px;">mmSign: mmWave-based Few-Shot Online Handwritten Signature Verification<br>
                    Mingda Han, Huanqi Yang, Tao Ni, <u><b>Di Duan</b></u>, Mengzhe Ruan, Yongliang Chen, Jia Zhang, Weitao Xu<br>
                      <i>ACM Transactions on Sensor Networks (<b>TOSN</b>)</i>
                  </li>
                </ul>

              </td>
            </tr>
          </tbody></table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <h2>Selected Publications</h2>
          <tr onmouseout="recon_stop()" onmouseover="recon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/recon.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/recon.png' width="160">
              </div>
              <script type="text/javascript">
                function recon_start() {
                  document.getElementById('recon_image').style.opacity = "1";
                }

                function recon_stop() {
                  document.getElementById('recon_image').style.opacity = "0";
                }
                recon_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">EarSE: Bringing Robust Speech Enhancement to COTS Headphones</span>
              <br>

              <u><b>Di Duan</b></u>,
              Yongliang Chen,
              Weitao Xu,
              Tianxing Li
              <br>
              <br>

              [<a href="papers/EarS_imwut24.pdf">paper</a>] [<a href="cite/EarSE.bib">cite</a>]
              <br>

              <p></p>
              <p>
                EarSE is a pioneer in exploring boom mic sensing, for the first time utilizing COTS headphones and boom/modular microphones to establish a stable acoustic sensing field across the user's face. It filters out ambient noise to obtain the wearer's clean speech in a multi-modality manner. 
              </p>
            </td>
          </tr>

          <tr onmouseout="recon_stop()" onmouseover="recon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/recon.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/recon.png' width="160">
              </div>
              <script type="text/javascript">
                function recon_start() {
                  document.getElementById('recon_image').style.opacity = "1";
                }

                function recon_stop() {
                  document.getElementById('recon_image').style.opacity = "0";
                }
                recon_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <span class="papertitle">EarSE: Bringing Robust Speech Enhancement to COTS Headphones</span>
              <br>

              <a href="https://raphaelduan.github.io/">Di Duan</a>,
              Yongliang Chen,
              Weitao Xu,
              Tianxing Li
              <br>

              [<a href="papers/EarS_imwut24.pdf">paper</a>]
              <br>

              <font color="red"> üèÜ Mark Weiser Best Paper Award (1/159)</font>
              <p></p>
              <p>
                EarSE is a pioneer in exploring boom mic sensing, for the first time utilizing COTS headphones and boom/modular microphones to establish a stable acoustic sensing field across the user's face. It filters out ambient noise to obtain the wearer's clean speech in a multi-modality manner. 
              </p>
            </td>
          </tr>
          </tbody></table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <h2>Services</h2>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
        <td width="75%" valign="center">
          <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
          <br>
          <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
          <br>
          <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
          <br>
          <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
          <br>
          <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
          <br>
          <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
        </td>
      </tr>
            
            

            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
